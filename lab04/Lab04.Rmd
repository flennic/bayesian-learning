---
title: "Bayesian Learning - Lab 04"
author: "Lakshidaa Saigiridharan (laksa656) and Maximilian Pfundstein (maxpf364)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: false
    number_sections: false
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(gridExtra)
library(coda)
library(rstan)

# RStan Setup
## Use multi-cores
options(mc.cores = parallel::detectCores())
## Reuse compiled binary
rstan_options(auto_write = TRUE)
```

# Time series models in Stan

**Exercise:** 

(a) Write a function in R that simulates data from the AR(1)-process
$$ x_t = \mu + \phi(x_{t-1} - \mu) + \epsilon_t,  \epsilon_t \sim N(0, \sigma^2)      $$
for given values of $\mu$, $\phi$ and $\sigma^2$. Start the process at $x_1$ = $\mu$ and then simulate values for $x_t$ for t = 2, 3 . . . , T and return the vector $x_{1:T}$ containing all time points.  Use $\mu$ = 10, $\sigma^2$ = 2 and T = 200 and look at some different realizations (simulations) of $x_{1:T}$ for values of $\phi$ between -1 and 1 (this is the interval of $\phi$ where the AR(1)-process is stable). Include a plot of at least one realization in the report. What effect does the value of $\phi$ have on $x_{1:T}$ ?

```{r}

ar_process = function(mu, tau, phi, sigma_sq) {
  
  # storing the values
  X = rep(NaN, tau)
  
  X[1] = mu
  
  for (i in 1:(tau - 1)) {
    X[i+1] = mu + phi *(X[i] - mu) + rnorm(n = 1, mean = 0, sd = sqrt(sigma_sq))
  }
  
  return(X)
}

```

```{r}

simulate_ar_process = function(mu, tau, phi, sigma_sq) {
  
  res_1 = ar_process(mu, tau, phi, sigma_sq)
  res_2 = ar_process(mu, tau, phi, sigma_sq)
  
  df = data.frame(x = 1:tau,
                  y1 = res_1,
                  y2 = res_2)
  
  p = ggplot(df) +
    geom_line(aes(x = x, y = y1), color = "#C70039") +
    geom_line(aes(x = x, y = y2), color = "#2E4053") +
    labs(title = paste("mu =", mu, "| T =", tau, "\nphi =", phi, " | sigma_sq = ", sigma_sq) , y = "mu",
    x = "Iteration", color = "Legend") +
    theme_minimal()
  
  return(p)
}

p1 = simulate_ar_process(mu = 10, tau = 200, phi = -0.5, sigma_sq = 2)
p2 = simulate_ar_process(mu = 10, tau = 200, phi = 0.75, sigma_sq = 2)
p3 = simulate_ar_process(mu = 10, tau = 200, phi = 0.9, sigma_sq = 2)
p4 = simulate_ar_process(mu = 10, tau = 200, phi = 1, sigma_sq = 2)

grid.arrange(p1, p2, p3, p4, nrow = 2)

```

**Answer:** Phi = 1 is random walk, if abs(phi) < 1 it's wide-sense stationary and if abs(phi) > 1 it's not stationary.

(b) Use your function from a) to simulate two AR(1)-processes, $x_{1:T}$ with $\phi$ = 0.3 and $y_{1:T}$ with $\phi$ = 0.95. Now, treat the values of $\mu$, $\phi$ and $\sigma^2$ as unknown and estimate them using MCMC. Implement Stan-code that samples from the posterior of the three parameters, using suitable non-informative priors of your choice. [Hint: Look at the time-series models examples in the Stan reference manual, and note the different parameterization used here.]

    (i) Report the posterior mean, 95% credible intervals and the number of effective posterior samples for the three inferred parameters for each of the simulated AR(1)-process. Are you able to estimate the true values?
    (ii) For each of the two data sets, evaluate the convergence of the samplers and plot the joint posterior of $\mu$ and $\phi$. Comments?
    
```{r, include = FALSE}

X = ar_process(mu = 10, tau = 200, phi = 0.3, sigma_sq = 2)
Y = ar_process(mu = 10, tau = 200, phi = 0.95, sigma_sq = 2)

stanModel = '

data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real phi;
  real<lower=0> sigma_sq;
}
model {
  y[2:N] ~ normal(mu + phi * y[1:(N - 1)], sqrt(sigma_sq));
}

'

stanModelX = stan(model_code = stanModel,
                  model_name = "AR_X",
                  data = list(N = length(X), y = X),
                  warmup = 1000,
                  iter = 2000)

stanModelY = stan(model_code = stanModel,
                  model_name = "AR_Y",
                  data = list(N = length(Y), y = Y),
                  warmup = 1000,
                  iter = 2000)

posteriorX = extract(stanModelX)
posterior_paramsX = As.mcmc.list(stanModelX)

posteriorY = extract(stanModelY)
posterior_paramsY = As.mcmc.list(stanModelY)

```

```{r}

stanModelX
summary(stanModelX, pars = c("mu", "phi", "sigma_sq"), probs = c(0.025, 0.975))$summary

plot(posterior_paramsX)
pairs(stanModelX)

```


```{r}

stanModelY
  summary(stanModelY, pars = c("mu", "phi", "sigma_sq"), probs = c(0.025, 0.975))$summary

plot(posterior_paramsY)
pairs(stanModelY)

```

```{r, echo=FALSE}

plotdf = data.frame(muX = posteriorX$mu,
                    phiX = posteriorX$phi,
                    muY = posteriorY$mu,
                    phiY = posteriorY$phi)

ggplot(plotdf)+
  geom_point(aes(x = muX, y = phiX), color = "black",
             fill = "#dedede", shape = 21) +
  labs(title = "Joint Posterior of mu and phi for process 1",
       y = "phi", x = "mu", color = "Legend") +
  theme_minimal()

ggplot(plotdf)+
  geom_point(aes(x = muY, y = phiY), color = "black",
             fill = "#dedede", shape = 21) +
  labs(title = "Joint Posterior of mu and phi for process 2",
       y = "phi", x = "mu", color = "Legend") +
  theme_minimal()

```


(c) The data `campy.dat` contain the number of cases of campylobacter infections in the north of the province Quebec (Canada) in four week intervals from January 1990 to the end of October 2000. It has 13 observations per year and 140 observations in total. Assume that the number of infections $c_t$ at each time point follows an independent Poisson distribution when conditioned on a latent AR(1)-process $x_t$, that is

$$c_t | x_t  \sim Poisson(exp(x_t))$$


where $x_t$ is an AR(1)-process as in a). Implement and estimate the model in Stan, using suitable priors of your choice. Produce a plot that contains both the data and the posterior mean and 95% credible intervals for the latent intensity $\theta_t = exp (x_t)$ over time. [Hint: Should $x_t$ be seen as data or parameters?]

First we import the data.

```{r}

campy_data = read.table("data/campy.dat", header=TRUE)

```

We define the following Stan model. Note that we basically have the same paramters as before. One new parameter is x, which represents the AR(1) process and it dependends on the previous parameters. We define the first x as a normal around $\mu$ with the prior for $\sigma$ as the variance. It will therefore heavily depend on the given data but as that's the best we know about the underlying process, we will go for it. For $\sigma^2$ we chose a chi squared prior as the variance has to be positve and a normal would not suit this. As we assume to not know that much about the prior we went for 1 degree of freedom and $\nu = 10$.

```{r}

stanModel = '

data {
  int<lower=0> N;
  int<lower=0> y[N];
}
parameters {
  real mu;
  real phi;
  real<lower=0> sigma_sq;
  vector[N] x;
}
model {

  // Priors
  mu ~ normal(5, 1);
  phi ~ normal(0, 10);
  sigma_sq ~ scaled_inv_chi_square(1, 10);
  
  // Model
  x[1] ~ normal(mu, sigma_sq);
  x[2:N] ~ normal(mu + phi * x[1:(N-1)], sqrt(sigma_sq));
  y[1:N] ~ poisson(exp(x[1:N]));
}

'

stanModelPoisson = stan(model_code = stanModel,
                  model_name = "AR_Poisson",
                  data = list(N = length(campy_data$c), y = campy_data$c),
                  warmup = 1000,
                  iter = 2000)

posteriorPoisson = extract(stanModelPoisson)

```

We will plot the mean of our samples. As they respresent $x_t$ which is equal to $\lambda$ which is in turn the mean of the Possion distribution. We can therefore directly take it as our mean and plot it. We use the `quantile()` function for obtaining the credible interval.

```{r}

ci = apply(exp(posteriorPoisson$x), 2, quantile, probs = c(0.025, 0.975))

df = data.frame(x = 1:length(campy_data$c),
                  real = campy_data$c,
                  simulated = exp(colMeans(posteriorPoisson$x)),
                  lower_ci = ci[1,],
                  upper_ci = ci[2,])

```

The plot looks as follows. The darker area indicates the 95% credible interval.

```{r, echo=FALSE}
  
ggplot(df) +
    geom_line(aes(x = x, y = real, color = "real")) +
    geom_line(aes(x = x, y = simulated, color = "simulated")) +
    geom_ribbon(aes(x = x, ymin = lower_ci, ymax = upper_ci), fill = "black",
              alpha = 0.25) +
    labs(title = "Simulated and Real Process" , y = "mu",
    x = "Iteration", color = "Legend") +
    scale_color_manual(values = c("#C70039", "#2E4053")) +
    theme_minimal()

```

(d) Now, assume that we have a prior belief that the true underlying intensity $\theta_t$ varies more smoothly than the data suggests. Change the prior for $\sigma^2$ so that it becomes informative about that the AR(1)-process increments $\epsilon_t$ should be small. Re-estimate the model using Stan with the new prior and produce the same plot as in c). Has the posterior for $\theta_t$ changed?

We choose a chi squared with 300 degress of freedom  and a very small $\nu$ to make the prior for $\sigma^2$ informative. We observe that the prediction curve is more flattened out and therefore generalizes better if we'd use it for making future predictions.

```{r}

stanModel = '

data {
  int<lower=0> N;
  int<lower=0> y[N];
}
parameters {
  real mu;
  real phi;
  real<lower=0> sigma_sq;
  vector[N] x;
}
model {

  // Priors
  mu ~ normal(5, 1);
  phi ~ normal(0, 10);
  sigma_sq ~ scaled_inv_chi_square(300, 0.1);
  
  // Model
  x[1] ~ normal(mu, sigma_sq);
  x[2:N] ~ normal(mu + phi * x[1:(N-1)], sqrt(sigma_sq));
  y[1:N] ~ poisson(exp(x[1:N]));
}

'

stanModelPoisson = stan(model_code = stanModel,
                  model_name = "AR_Poisson",
                  data = list(N = length(campy_data$c), y = campy_data$c),
                  warmup = 1000,
                  iter = 2000)

posteriorPoisson = extract(stanModelPoisson)

```


```{r}

ci = apply(exp(posteriorPoisson$x), 2, quantile, probs = c(0.025, 0.975))

df = data.frame(x = 1:length(campy_data$c),
                  real = campy_data$c,
                  simulated = exp(colMeans(posteriorPoisson$x)),
                  lower_ci = ci[1,],
                  upper_ci = ci[2,])

```

```{r, echo=FALSE}

ggplot(df) +
    geom_line(aes(x = x, y = real, color = "real")) +
    geom_line(aes(x = x, y = simulated, color = "simulated")) +
    geom_ribbon(aes(x = x, ymin = lower_ci, ymax = upper_ci), fill = "black",
              alpha = 0.25) +
    labs(title = "Simulated and Real Process" , y = "mu",
    x = "Iteration", color = "Legend") +
    scale_color_manual(values = c("#C70039", "#2E4053")) +
    theme_minimal()

```

# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```
